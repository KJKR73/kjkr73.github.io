# Karanjot Singh Saggu
## Technical Skills
**Programming Languages**: Python | JavaScript | Java <br>
**Artificial Intelligence**: Natural Language Processing (NLP) | Computer Vision (CV) | Recommender Systems | Graph Neural Networks | Retrieval Augmented Generation <br>
**Visualization**: Matplotlib | Seaborn | NetworkX | Power BI <br>
**Cloud**: Amazon Web Services (AWS) | Microsoft Azure | Docker | Kubernetes | MLOps <br>
**Frameworks**: PyTorch | TensorFlow | NumPy | Pandas | Scikit-Learn | SciPy | Jupyter <br>
**Databases**: SQL | MongoDB | Redshift <br>
**Deployment**: GitHub Actions | Github | FastAPI <br>
**Misc**: spaCy | JIRA | Airflow | Spark | Node.js | OpenCV | Linux | MLflow | HuggingFace | JIRA | DVC <br>

## Work Experience
**Data Scientist and Complex System Engineer (Co-op)**	(April 2023 – December 2023)<br>
*National Research Council* | Ottawa, Canada<br>
•	Finetuned Llama-2-based Large Language Models for text infilling and editing, achieving an accuracy of 92%.<br>
•	Pioneered a framework with GPT-2, LangChain Vector Stores, and RAG for synthesizing protein sequences.<br>
•	Utilized AWS Redshift to design a data warehouse, PySpark, and SQL for performing ETL on text data sizing to 100GB.<br>
•	Deployed a scalable pipeline for the project using AWS Sagemaker, Kubeflow (Kubernetes), and PyTorch Distributed.<br>
•	Utilized MLflow, PowerBI, and Matplotlib to summarize and visualize weekly reports.<br>


**Data Scientist**	(June 2021 – February 2023)<br>
*Tatras Data Services* | Delhi, India<br>
•	Created a Vision Transformer, Optical Character Recognition (OCR), and BERT solution for a Fortune 100 company to classify and parse unstructured documents increasing information extraction accuracy by 51%.<br>
•	Developed an end-to-end Document Processor for a US Inspection firm through actionable Proof of Concepts (PoCs) using Layout Detection, Text Recognition, and Sentence LLMs accelerating invoice generation by 96%.<br>
•	Architected comprehensive CI/CD and ETL pipelines using PostgreSQL, Azure/AWS, GitHub Actions, Docker, Airflow, and PySpark for automatic Data Injection, Model Training, and Evaluation by the client.<br>
•	Engineered a database scheme and API to search over 10 million text documents stored in multiple MongoDB databases using a custom MapReduce implementation reducing searching time from 800ms to 200ms per query.<br>
•	Conducted rigorous QA and User Acceptance Testing (UAT) for the above projects to improve stability by 5x.<br>
•	Leveraged dashboarding tools like PowerBI, D3.js and Matplotlib to summarize quantitative results to stakeholders.<br>


**Data Scientist Intern** 	(January 2021 – June 2021)<br>
*Sabudh Foundation* | Delhi, India<br>
•	Crafted a News Recommendation engine based on Neural Network collaborative filtering, spaCy Named Entity Recognition (NER), and statistical modeling fetching data from a custom-build news crawler API reaching user satisfaction of 88%.<br>
•	Employed Selenium-based crawler to scrape information and Pandas to analyze news and song lyrics from public websites.<br>
•	Leveraged Gradient Boosting, Regression models, and PowerBI to analyze trained models and generate insights.<br>

## Education
**Masters of Computer Science – Applied Artificial Intelligence (completed)**	<br>
September 2022 – April 2024 <br>
*University of Ottawa* | Ottawa, Ontario, Canada	GPA: 3.94/4.00 <br>
Teaching Assistant: Python Programming, Java Programming, and Cybersecurity <br> <br>
**Bachelor of Technology in Computer Science and Engineering** <br>
August 2017 – June 2021 <br>
*Punjabi University* | Punjab, India	GPA: 9.05/10.00 <br>




